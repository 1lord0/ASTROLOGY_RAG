import streamlit as st
import os
import google.generativeai as genai
from google.generativeai import GenerativeModel 
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

# -----------------------------
# 0) API VE KÃœTÃœPHANE AYARLARI
# -----------------------------
if "GEMINI_API_KEY" not in st.secrets:
    st.error("âŒ HATA: 'GEMINI_API_KEY', Streamlit Secrets'ta tanÄ±mlanmalÄ±dÄ±r.")
    st.stop()
    
genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
llm = GenerativeModel("gemini-1.5-flash")

# -----------------------------
# 1) CHROMA DB LOAD (Embedding Lazy Loading)
# -----------------------------
DB_PATH = "chroma_db"
if not os.path.exists(DB_PATH):
    st.error("âŒ HATA: Chroma DB ('chroma_db' klasÃ¶rÃ¼) bulunamadÄ±.")
    st.stop()

# ğŸ”¥ Embedding modelini sadece gerektiÄŸinde yÃ¼kle
@st.cache_resource
def get_embeddings():
    """Embeddings'i cache'le - bir kez yÃ¼kle"""
    return HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

@st.cache_resource
def get_vectordb():
    """VektÃ¶r veritabanÄ±nÄ± cache'le"""
    emb = get_embeddings()
    return Chroma(
        persist_directory=DB_PATH,
        embedding_function=emb
    )

# -----------------------------
# 2) RAG PIPELINE
# -----------------------------
def ask_rag(question):
    """KullanÄ±cÄ± sorusuna RAG ile cevap verir."""
    
    try:
        db = get_vectordb()
        
        # Direkt text ile arama (embedding hesaplanmÄ±ÅŸ)
        results = db.similarity_search(question, k=3)
        
        # Context oluÅŸtur
        context = "\n\n".join([chunk.page_content for chunk in results])
        
        # Prompt oluÅŸtur
        prompt = f"""Sen bir astroloji uzmanÄ±sÄ±n. AÅŸaÄŸÄ±daki bilgileri kullanarak soruyu yanÄ±tla.

BAÄLAM:
{context}

SORU: {question}

YANIT (TÃ¼rkÃ§e ve detaylÄ±):"""
        
        # Gemini API Ã§aÄŸrÄ±sÄ±
        response = llm.generate_content(prompt)
        
        return response.text, results
    
    except Exception as e:
        st.error(f"Model yÃ¼kleme hatasÄ±: {str(e)}")
        st.info("ğŸ’¡ LÃ¼tfen Python 3.11 kullanÄ±n veya packages.txt ekleyin")
        return None, []

# -----------------------------
# 3) STREAMLIT UI
# -----------------------------
st.title("ğŸ”® Astrology RAG Chatbot")
st.write("Astroloji hakkÄ±nda her ÅŸeyi sorabilirsiniz. Gemini + ChromaDB ile gÃ¼Ã§lendirilmiÅŸtir.")

question = st.text_input("Sorunuz:")

if st.button("Sorgula") or question:
    if not question or not question.strip():
        st.warning("âš ï¸ LÃ¼tfen geÃ§erli bir soru girin.")
    else:
        with st.spinner("YÄ±ldÄ±zlara danÄ±ÅŸÄ±lÄ±yor..."):
            answer, chunks = ask_rag(question)
            
            if answer:
                st.subheader("ğŸŒŸ Cevap")
                st.write(answer)
                
                with st.expander("ğŸ” Kaynak DÃ¶kÃ¼manlar"):
                    for i, c in enumerate(chunks, 1):
                        st.markdown(f"**Kaynak {i}:**")
                        st.text(c.page_content[:300] + "...")
                        st.divider()
